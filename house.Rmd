---
title: "house"
author: "Koki Ando"
date: "5/3/2019"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


[House Price Prediction](https://www.kaggle.com/c/house-prices-advanced-regression-techniques/data)

```{r}
library(tidyverse)
test = read.csv("test.csv")
train = read.csv("train.csv")
```

# EDA

```{r}
dim(train)
```

## Summary of train

```{r}
# summary(train)
```

## Sale Price

```{r}
train %>% 
  ggplot(aes(SalePrice)) +
  geom_histogram(bins = 50)
```

```{r}
summary(train$SalePrice)
```

## Missing values

```{r}
apply(train, 2, prop_miss)
```


### Dealing with Missing values

#### Combine datasets

```{r}
test$SalePrice = 0
train$Train = "YES"
test$Train = "NO"
merged_dat = rbind(train, test)
```

#### Check NAs again

```{r}
prop_miss(merged_dat)
```

```{r}
library(naniar)
library(simputation)
sort(apply(merged_dat, 2, n_miss), decreasing = T)
```

```{r}
merged_dat %>% 
  miss_var_summary() %>%
  filter(n_miss >= 1 & n_miss < 5) %>% 
  filter(variable %in% vars)
```

```{r}
vars = c("MSZoning", "Utilities", "BsmtFullBath", "BsmtHalfBath", "Functional", "Exterior1st", "Exterior2nd", 
         "BsmtFinSF1", "BsmtFinSF2", "BsmtUnfSF", "TotalBsmtSF", "Electrical", "KitchenQual", "GarageCars", "GarageArea", "SaleType")
```

```{r}
merged_dat %>% 
  select(vars) %>% 
  str()
```

```{r}
vars_int = c("BsmtFullBath", "BsmtHalfBath", "BsmtFinSF1", "BsmtFinSF2", "BsmtUnfSF", "TotalBsmtSF", "GarageCars", "GarageArea")
vars_factor = c("MSZoning", "Utilities", "Functional", "Exterior1st", "Exterior2nd", "Electrical", "KitchenQual", "SaleType")
merged_dat = impute_mean_at(merged_dat, vars_int)
merged_dat = merged_dat %>% 
  impute_cart(MSZoning ~ .) %>% 
  impute_cart(Utilities ~ .) %>% 
  impute_cart(Functional ~ .) %>% 
  impute_cart(Exterior1st ~ .) %>% 
  impute_cart(Exterior2nd ~ .) %>% 
  impute_cart(Electrical ~ .) %>% 
  impute_cart(KitchenQual ~ .) %>% 
  impute_cart(SaleType ~ .)
```

# Check NAs

```{r}
miss_var_summary(merged_dat)
```



## Top 7 missing values

```{r}
miss_var_summary(merged_dat)
```

```{r}
top_missing_vats = c("PoolQC", "MiscFeature", "Alley", "Fence", "FireplaceQu", "LotFrontage", "GarageYrBlt")
merged_dat %>% 
  select(top_missing_vats, SalePrice) %>% 
  str()
```


## Deeper EDA

### PoolQC

```{r}
merged_dat %>% 
  ggplot(aes(SalePrice, PoolQC)) +
  geom_boxplot()
```

```{r}
merged_dat %>% 
  bind_shadow() %>% 
  ggplot(aes(SalePrice, colour = PoolQC_NA)) +
  geom_density()
```



```{r}
nums <- unlist(lapply(merged_dat, is.numeric))  
res <- cor(merged_dat[, nums])
round(res, 2) %>% tail(1)
```

```{r}
merged_dat %>% 
  bind_shadow() %>% 
  ggplot(aes(TotalBsmtSF, SalePrice, colour = Alley_NA)) +
  geom_point(alpha = .4)
```




```{r}
merged_dat = select(merged_dat, -PoolQC, -MiscFeature, -Alley, -Fence)
merged_dat = select(merged_dat, -Fence)
```


```{r}
prop_miss(merge_dat)
miss_var_summary(select(merged_dat, contains("Garage")))
```


## Garage

```{r}
merged_dat %>% 
  select(contains("Garage"), SalePrice) %>% 
  str()
```


```{r}
merged_dat %>% 
  select(contains("Garage"), SalePrice) %>% 
  # bind_shadow() %>% 
  add_label_missings() %>% 
  ggplot(aes(GarageYrBlt, SalePrice), alpha = .4) +
  geom_point() +
  geom_miss_point()
```


```{r}
merged_dat %>% 
  ggplot(aes(GarageYrBlt, SalePrice)) +
  geom_point() +
  geom_miss_point()
```

### Linear Model imputation

```{r}
merged_dat = impute_lm(merged_dat, GarageYrBlt ~ SalePrice)
```

```{r}
nums <- unlist(lapply(merged_dat, is.numeric))  
res <- cor(merged_dat[, nums])
round(res, 2)[26:27,]
```


```{r}
merged_dat %>% 
  select(contains("Garage"), SalePrice) %>% 
  ggplot(aes(GarageYrBlt, SalePrice)) +
  geom_point() +
  geom_miss_point()
```


### Garage Type/Qual/Cond/Finish

```{r}
merged_dat %>% 
  ggplot(aes(GarageFinish)) +
  geom_bar(stat = "count")
# merged_dat %>% 
#   ggplot(aes(GarageQual, SalePrice, colour = GarageQual)) +
#   geom_boxplot()
# 
# merged_dat %>% 
#   ggplot(aes(SalePrice, colour = GarageQual)) +
#   geom_density()
```

##### Mode Imputation

```{r}
merged_dat[is.na(merged_dat$GarageType),]$GarageType = "Attchd"
merged_dat[is.na(merged_dat$GarageQual),]$GarageQual = "TA"
merged_dat[is.na(merged_dat$GarageCond),]$GarageCond = "TA"
merged_dat[is.na(merged_dat$GarageFinish),]$GarageFinish = "Unf"
```




### Basement

```{r}
select(merged_dat, contains("Bsmt"), SalePrice) %>% 
  head()
```



```{r}
merged_dat %>% 
  ggplot(aes(BsmtQual)) +
  geom_bar(stat = "count")
```

#### Mode Imputation

```{r}
merged_dat[is.na(merged_dat$BsmtFinType1),]$BsmtFinType1 = "GLQ"
merged_dat[is.na(merged_dat$BsmtFinType2),]$BsmtFinType2 = "Unf"
merged_dat[is.na(merged_dat$BsmtCond),]$BsmtCond = "TA"
merged_dat[is.na(merged_dat$BsmtExposure),]$BsmtExposure = "No"
merged_dat[is.na(merged_dat$BsmtQual),]$BsmtQual = "TA"
```


## FireplaceQu

```{r}
str(merged_dat$FireplaceQu)
merged_dat %>% 
  ggplot(aes(FireplaceQu)) +
  geom_bar(stat = "count")
```

##### Mode imputation

```{r}
merged_dat[is.na(merged_dat$FireplaceQu),]$FireplaceQu = "Gd"
```


#### MasVnr

```{r}
merged_dat %>% 
  select(SalePrice, contains("MasVnr")) %>% 
  ggplot(aes(SalePrice, MasVnrArea)) +
  geom_point() +
  geom_miss_point() +
  facet_wrap(~MasVnrType)
```

```{r}
merged_dat %>% 
  ggplot(aes(MasVnrType)) +
  geom_bar(stat = "count")
```


```{r}
merged_dat[is.na(merged_dat$MasVnrType), ]$MasVnrArea = 0
merged_dat[is.na(merged_dat$MasVnrType), ]$MasVnrType = "None"
```



```{r}
prop_miss(merged_dat)
```


```{r}
miss_var_summary(merged_dat)
```

```{r}
merged_dat %>% 
  ggplot(aes(SalePrice, LotFrontage)) +
  geom_point() +
  geom_miss_point()
  bind_shadow() %>% 
  ggplot(aes(LotFrontage, colour = LotFrontage_NA)) +
  geom_density()
```

```{r}
merged_dat[is.na(merged_dat$LotFrontage), ]$LotFrontage = 0
```

```{r}
names(merged_dat)
```

削除したカラムから特徴量を作成
Conducting Cross Validation
from caret tunelength

# caret::dummyVars -Feature Engineering-

+ [dummyVarsでダミー変数化](https://qiita.com/daifuku_mochi2/items/2885dc02ac4bfc284e06)

```{r}
library(caret)
merged_dat_dummy = dummyVars(~., data = merged_dat)
merged_dat_dummy2 = as.data.frame(predict(merged_dat_dummy, merged_dat))
head(merged_dat_dummy2)
# train_treat = filter(merged_dat_dummy2, TrainYES == 1)
# test_treat = filter(merged_dat_dummy2, TrainNO == 1)

train_treat = merged_dat_dummy2 %>% 
  filter(TrainYES == 1) %>% 
  select(-contains("Train"), -SalePrice)
test_treat = merged_dat_dummy2 %>% 
  filter(TrainYES == 0) %>% 
  select(-contains("Train"), -SalePrice)
train = select(train, -pred_xgb)
```

## Finding the optimal number of trees

```{r}
library(xgboost)
cv <- xgb.cv(data = as.matrix(train_treat), 
            label = train$SalePrice,
            nrounds = 100,
            nfold = 5,
            objective = "reg:linear",
            eta = .3,
            max_depth = 6,
            early_stopping_rounds = 10,
            verbose = 0
)
elog <- as.data.frame(cv$evaluation_log)

elog %>% 
   summarize(ntrees.train = which.min(train_rmse_mean),
             ntrees.test  = which.min(test_rmse_mean))

```


```{r}
model_xgb <- xgboost(data = as.matrix(train_treat), 
                     label = train$SalePrice,
                     nrounds = 25,
                      objective = "reg:linear",
                      eta = .3,
                      depth = 6,
                      verbose = 0
)

# train$pred_xgb = predict(model_xgb, as.matrix(train_treat))
# train %>% 
#   mutate(xgb_residuals = pred_xgb - SalePrice) %>% 
#   summarise(xgb_rmse = sqrt(mean(xgb_residuals^2)))
```

```{r}
test = select(test, -SalePrice)
test$SalePrice = predict(model_xgb, as.matrix(test_treat))
# select(test, Id, SalePrice)
```

# Model Accuracy check

```{r}
train_acc_test = train
train_acc_test$xgb_pred = predict(model_xgb, as.matrix(train_treat))
train_acc_test %>% 
  mutate(xgb_residuals = xgb_pred - SalePrice) %>% 
  summarise(sqrt(mean(xgb_residuals^2)))
```


# Baseline prediction

```{r}
train = merged_dat %>% 
  filter(Train == "YES") %>% 
  select(-Train)
test = merged_dat %>% 
  filter(Train == "NO") %>% 
  select(-Train, -SalePrice)
```



```{r}
library(randomForest)
library(gbm)
model_rf = randomForest(SalePrice ~ ., data = train,
                         ntree = 500, importance = TRUE)
model_rpart = rpart(SalePrice ~ ., data = train)
```

```{r}
library(ranger)

model_rf_ranger = ranger(SalePrice ~ ., data = train,
                         num.trees = 500, respect.unordered.factors = "order")
```



## Model Accuracy 

```{r}
library(modelr)
mae(model = model_rf, data = train)
```

5/6: 6598.363


```{r}
accuracy_check_model = train
accuracy_check_model$pred_rf = predict(model_rf, train)
accuracy_check_model$pred_rf_ranger = predict(model_rf_ranger, train)$predictions
accuracy_check_model$pred_rpart = predict(model_rpart, train)
```

```{r}
accuracy_check_model %>% 
  select(SalePrice, contains("pred")) %>% 
  mutate(rf_residuals = pred_rf - SalePrice,
         ranger_residuals = pred_rf_ranger - SalePrice,
         rpart_residuals = pred_rpart - SalePrice) %>% 
  summarise(
    rf_rmse = sqrt(mean(rf_residuals^2)),
    ranger_rmse = sqrt(mean(ranger_residuals^2)),
    rpart_rmse = sqrt(mean(rpart_residuals^2))
    )
```



# Hyperparameter Tuning

# Model Submission

```{r}
test$SalePrice = predict(model_rpart, test)
test$SalePrice = predict(model_rf, test)
# submission = select(test, Id, SalePrice)
# write.csv(submission, "submission.csv", row.names = FALSE)
```


