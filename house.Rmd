---
title: "house"
author: "Koki Ando"
date: "5/3/2019"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


[House Price Prediction](https://www.kaggle.com/c/house-prices-advanced-regression-techniques/data)

# Preparation

```{r}
library(tidyverse)
library(naniar)
test = read.csv("test.csv")
train = read.csv("train.csv")

# Combining datasets
test$SalePrice = 0
train$Train = "YES"
test$Train = "NO"
merged_dat = rbind(train, test)
```

# EDA

```{r}
dim(train)
```

## SalePrice Distribution

```{r}
train %>% 
  ggplot(aes(SalePrice)) +
  geom_histogram(bins = 50)
summary(train$SalePrice)
```

# Dealing with Missing values

## Check NAs

```{r}
prop_miss(merged_dat)
```

Original proportion of missing values: 0.05834357

```{r}
merged_dat %>% 
  miss_var_summary()
```

```{r}
vars = c("MSZoning", "Utilities", "BsmtFullBath", "BsmtHalfBath", "Functional", "Exterior1st", "Exterior2nd", 
         "BsmtFinSF1", "BsmtFinSF2", "BsmtUnfSF", "TotalBsmtSF", "Electrical", "KitchenQual", "GarageCars", "GarageArea", "SaleType")
merged_dat %>% 
  select(vars) %>% 
  str()
```

```{r}
vars_int = c("BsmtFullBath", "BsmtHalfBath", "BsmtFinSF1", "BsmtFinSF2", "BsmtUnfSF", "TotalBsmtSF", "GarageCars", "GarageArea")
vars_factor = c("MSZoning", "Utilities", "Functional", "Exterior1st", "Exterior2nd", "Electrical", "KitchenQual", "SaleType")
```

### Mean Imputation

```{r}
merged_dat = impute_mean_at(merged_dat, vars_int)
```

### Linear model Imputation

```{r}
merged_dat = merged_dat %>% 
  impute_cart(MSZoning ~ .) %>% 
  impute_cart(Utilities ~ .) %>% 
  impute_cart(Functional ~ .) %>% 
  impute_cart(Exterior1st ~ .) %>% 
  impute_cart(Exterior2nd ~ .) %>% 
  impute_cart(Electrical ~ .) %>% 
  impute_cart(KitchenQual ~ .) %>% 
  impute_cart(SaleType ~ .)
```


## Check NAs again

```{r}
prop_miss(merged_dat)
```

Second check: 0.05824748

```{r}
top_missing_vars = c("PoolQC", "MiscFeature", "Alley", "Fence", "FireplaceQu", "LotFrontage", "GarageYrBlt")
merged_dat %>% 
  select(top_missing_vars, SalePrice) %>% 
  str()
```

### PoolQC

```{r}
merged_dat$PoolQC = as.character(merged_dat$PoolQC)
merged_dat[is.na(merged_dat$PoolQC),]$PoolQC = "Hi"
merged_dat$PoolQC = factor(merged_dat$PoolQC)
```

### MiscFeature

```{r}
merged_dat$MiscFeature = as.character(merged_dat$MiscFeature)
merged_dat[is.na(merged_dat$MiscFeature),]$MiscFeature = "High"
merged_dat$MiscFeature = factor(merged_dat$MiscFeature)
```

### Alley

```{r}
merged_dat$Alley = as.character(merged_dat$Alley)
merged_dat[is.na(merged_dat$Alley),]$Alley = "HighSP"
merged_dat$Alley = factor(merged_dat$Alley)
```

### Fence

```{r}
merged_dat$Fence = as.character(merged_dat$Fence)
merged_dat[is.na(merged_dat$Fence),]$Fence = "Hig"
merged_dat$Fence = factor(merged_dat$Fence)
```

### FireplaceQu

```{r}
merged_dat$FireplaceQu = as.character(merged_dat$FireplaceQu)
merged_dat[is.na(merged_dat$FireplaceQu),]$FireplaceQu = "Hg"
merged_dat$FireplaceQu = factor(merged_dat$FireplaceQu)
```


```{r}
prop_miss(merged_dat)
miss_var_summary(select(merged_dat, contains("Garage")))
```

### Garage

#### Linear Model imputation

```{r}
merged_dat = impute_lm(merged_dat, GarageYrBlt ~ SalePrice)
```

```{r}
nums <- unlist(lapply(merged_dat, is.numeric))  
res <- cor(merged_dat[, nums])
round(res, 2)[26:27,]
```


#### Garage Type/Qual/Cond/Finish

##### Mode Imputation

```{r}
merged_dat[is.na(merged_dat$GarageType),]$GarageType = "Attchd"
merged_dat[is.na(merged_dat$GarageQual),]$GarageQual = "TA"
merged_dat[is.na(merged_dat$GarageCond),]$GarageCond = "TA"
merged_dat[is.na(merged_dat$GarageFinish),]$GarageFinish = "Unf"
```

### Basement

#### Mode Imputation

```{r}
merged_dat[is.na(merged_dat$BsmtFinType1),]$BsmtFinType1 = "GLQ"
merged_dat[is.na(merged_dat$BsmtFinType2),]$BsmtFinType2 = "Unf"
merged_dat[is.na(merged_dat$BsmtCond),]$BsmtCond = "TA"
merged_dat[is.na(merged_dat$BsmtExposure),]$BsmtExposure = "No"
merged_dat[is.na(merged_dat$BsmtQual),]$BsmtQual = "TA"
```

#### MasVnr

```{r}
merged_dat[is.na(merged_dat$MasVnrType), ]$MasVnrType = "None"
merged_dat[is.na(merged_dat$MasVnrArea), ]$MasVnrArea = 0
```

#### LotFrontage

```{r}
merged_dat[is.na(merged_dat$LotFrontage), ]$LotFrontage = 0
```

```{r}
prop_miss(merged_dat)
```

Final check: 0

# caret::dummyVars -Feature Engineering-

+ [dummyVarsでダミー変数化](https://qiita.com/daifuku_mochi2/items/2885dc02ac4bfc284e06)

```{r}
library(caret)
merged_dat_dummy = dummyVars(~., data = merged_dat)
merged_dat_dummy2 = as.data.frame(predict(merged_dat_dummy, merged_dat))
head(merged_dat_dummy2)
# train_treat = filter(merged_dat_dummy2, TrainYES == 1)
# test_treat = filter(merged_dat_dummy2, TrainNO == 1)

train_treat = merged_dat_dummy2 %>% 
  filter(TrainYES == 1) %>% 
  select(-contains("Train"), -SalePrice)
test_treat = merged_dat_dummy2 %>% 
  filter(TrainYES == 0) %>% 
  select(-contains("Train"), -SalePrice)
# train = select(train, -pred_xgb)
```

## Finding the optimal number of trees

```{r}
library(xgboost)
cv <- xgb.cv(data = as.matrix(train_treat), 
            label = train$SalePrice,
            nrounds = 100,
            nfold = 5,
            objective = "reg:linear",
            eta = .3,
            max_depth = 6,
            early_stopping_rounds = 10,
            verbose = 0
)
elog <- as.data.frame(cv$evaluation_log)

elog %>% 
   summarize(ntrees.train = which.min(train_rmse_mean),
             ntrees.test  = which.min(test_rmse_mean))

```


```{r}
model_xgb <- xgboost(data = as.matrix(train_treat), 
                     label = train$SalePrice,
                     nrounds = 42,
                      objective = "reg:linear",
                      eta = .3,
                      depth = 6,
                      verbose = 0
)

# train$pred_xgb = predict(model_xgb, as.matrix(train_treat))
# train %>% 
#   mutate(xgb_residuals = pred_xgb - SalePrice) %>% 
#   summarise(xgb_rmse = sqrt(mean(xgb_residuals^2)))
```

```{r}
test = select(test, -SalePrice)
test$SalePrice = predict(model_xgb, as.matrix(test_treat))
# select(test, Id, SalePrice)
```

# Model Accuracy check

```{r}
train_acc_test = train
train_acc_test$xgb_pred = predict(model_xgb, as.matrix(train_treat))
train_acc_test %>% 
  mutate(xgb_residuals = xgb_pred - SalePrice) %>% 
  summarise(sqrt(mean(xgb_residuals^2)))
```

1: 7715.745
2: 5383.883

# Baseline prediction

```{r}
train = merged_dat %>% 
  filter(Train == "YES") %>% 
  select(-Train)
test = merged_dat %>% 
  filter(Train == "NO") %>% 
  select(-Train, -SalePrice)
```



```{r}
model_rf = randomForest(SalePrice ~ ., data = train,
                         ntree = 500, importance = TRUE)
model_rpart = rpart(SalePrice ~ ., data = train)
```

```{r}
library(ranger)

model_rf_ranger = ranger(SalePrice ~ ., data = train,
                         num.trees = 500, respect.unordered.factors = "order")
```



## Model Accuracy 

```{r}
accuracy_check_model = train
accuracy_check_model$pred_rf = predict(model_rf, train)
accuracy_check_model$pred_rf_ranger = predict(model_rf_ranger, train)$predictions
accuracy_check_model$pred_rpart = predict(model_rpart, train)
```

```{r}
accuracy_check_model %>% 
  select(SalePrice, contains("pred")) %>% 
  mutate(rf_residuals = pred_rf - SalePrice,
         ranger_residuals = pred_rf_ranger - SalePrice,
         rpart_residuals = pred_rpart - SalePrice) %>% 
  summarise(
    rf_rmse = sqrt(mean(rf_residuals^2)),
    ranger_rmse = sqrt(mean(ranger_residuals^2)),
    rpart_rmse = sqrt(mean(rpart_residuals^2))
    )
```

rmse
rf:       11268.29
ranger:   12330.57
rpart:    38464.11
xgboost:   5383.883



# Hyperparameter Tuning

# Model Submission

```{r}
test$SalePrice = predict(model_rpart, test)
test$SalePrice = predict(model_rf, test)
# submission = select(test, Id, SalePrice)
# write.csv(submission, "submission.csv", row.names = FALSE)
```


